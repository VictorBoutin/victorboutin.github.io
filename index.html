<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Victor Boutin</title>

  <meta name="author" content="Victor Boutin">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table class="intro"
    style="max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Victor Boutin</name>
                  </p>
                  <p>Hi ðŸ‘‹ </p>
                  <p>I am Victor Boutin, a researcher in artificial intelligence and computational neuroscience. I obtained a PhD in the 
                    <a href="https://www.int.univ-amu.fr">Institute of Neuroscience of Marseille </a>, at the 
                    Aix-Marseille university. My PhD was supervised by 
                    <a href="https://laurentperrinet.github.io">Laurent University</a>. I did my post-doc with 
                    <a href="https://serre-lab.clps.brown.edu/person/thomas-serre">Thomas Serre</a> at
                    <a href="https://aniti.univ-toulouse.fr/en/" class="aniti-txt">ANITI (Toulouse, France)</a>
                    & <a href="https://serre-lab.clps.brown.edu/" class="brown-txt">Brown University (Boston, USA)</a>.
                  </p>
                  <p>
                    I'm interested in <b>Explainability (XAI)</b>, computer vision, optimization and certification of
                    neural network systems.
                    <!-- Most of my research is focused on reverse engineer the key strategies used by current neural networks. -->
                    Most of my research involves reverse engineering existing neural networks to understand their
                    critical strategies.
                    In my free time I also like to contribute to open source project
                    in particular, I am the author of <a href="https://github.com/deel-ai/xplique">Xplique</a> .
                    <!-- I like to believe that understanding is the reflection of creativity. -->
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:thomas_fel@brown.edu">thomas_fel@brown.edu</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=1m5Mlx4AAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/napoolar">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/fel-thomas/">Github</a>
                  </p>

                  <p style="text-align:center">
                  <div><b>NEWS</b></div>
                  <ul>
                    <li>09/2022 : 3 papers accepted at NeurIPS!</li>
                    <li>07/2022 : Oral presentation of 'Don't lie to me' at ICML Workshop</li>
                    <li>06/2022 : Presentation of Xplique at CVPR</li>
                    <li>12/2021 : 'How Good is your Explanation?' accepted at WACV 2022!</li>
                    <li>09/2021 : Sobol Attribution Method accepted at NeurIPS!</li>
                    <li>04/2021 : Official start of the thesis</li>
                  </ul>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%" class="picture">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="research-table"
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <!-- CRAFT -->
              <tr>
                <td>
                  <div>
                    <img src='images/craft_miniature.jpg' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    CRAFT: Concept Recursive Activation FacTorization
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel*</strong>, Agustin Picard*, Louis BÃ©thune*, Thibaut Boissin*,
                    David Vigouroux, Julien Colin, RÃ©mi CadÃ¨ne, Thomas Serre
                  </div>
                  <div class="conference">Under review, 2022</div>
                  <div class="links">
                    <!-- <a href="https://arxiv.org/abs/2112.04417">Article</a> -->
                  </div>

                  <p class="resume">
                    Despite their considerable potential, concept-based explainability methods have received relatively
                    little attention,
                    and explaining whatâ€™s driving modelsâ€™ decisions and where itâ€™s located in the input is still an open
                    problem.
                    To tackle this, we revisit unsupervised concept extraction techniques for explaining the decisions
                    of deep neural networks and present CRAFT
                    â€“ a framework to generate concept-based explanations for understanding individual predictions and
                    the modelâ€™s high-level logic for whole classes.
                    CRAFT takes advantage of a novel method for recursively decomposing higher-level concepts into more
                    elementary ones, combined with a
                    novel approach for better estimating the importance of identified concepts with Sobol indices.
                    Furthermore, we show how implicit differentiation can be used to generate concept-wise attribution
                    explanations for individual images.
                    We further demonstrate through fidelity metrics that our proposed concept importance estimation
                    technique is more faithful to the model than previous methods,
                    and, through human psychophysic experiments, we confirm that our recursive decomposition can
                    generate meaningful and accurate concepts.
                    Finally, we illustrate CRAFTâ€™s potential to enable the understanding of predictions of trained
                    models on multiple use-cases by
                    producing meaningful concept-based explanations.
                  </p>
                </td>
              </tr>



              <!-- Harmonization -->
              <tr>
                <td>
                  <div>
                    <img src='images/harmonization_miniature.jpg' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    Harmonizing the object recognition strategies of deep neural networks with humans
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel*</strong>, Ivan Felipe*, Drew Linsley*, Thomas Serre
                  </div>
                  <div class="conference"><span class="badge">NeurIPS</span> Proceedings of the Conference on Neural
                    Information Processing Systems, 2022</div>
                  <div class="links">
                    <!-- <a href="https://arxiv.org/abs/2112.04417">Article</a> -->
                    <a href="https://serre-lab.github.io/Harmonization/">Website</a> /
                    <a href="https://github.com/serre-lab/harmonization">Code</a>
                  </div>

                  <p class="resume">
                    The many successes of deep neural networks (DNNs) over the past decade have largely been driven by
                    computational scale rather than insights from biological intelligence.
                    Here, we explore if these trends have also carried concomitant improvements in explaining the visual
                    strategies humans rely on for object recognition.
                    We do this by comparing two related but distinct properties of visual strategies in humans and DNNs:
                    where they believe important visual features are in images and how they use those features to
                    categorize objects.
                    Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and
                    the how of human visual strategies for object recognition on those images,
                    we find a systematic trade-off between DNN categorization accuracy and alignment with human visual
                    strategies for object recognition.
                    State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy
                    improves.
                    We rectify this growing issue with our harmonization loss: a general-purpose training routine that
                    both aligns DNN and human visual strategies and improves categorization accuracy.
                    Our work represents the first demonstration that the scaling laws that are guiding the design of
                    DNNs today have also produced worse models of human vision.
                  </p>
                </td>
              </tr>

              <!-- Human Meta-predictor -->
              <tr>
                <td>
                  <div>
                    <img src='images/metapred_miniature.jpg' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability
                    Methods
                  </div>
                  <div class="authors">
                    Julien Colin*, <strong>Thomas Fel*</strong>, RÃ©mi CadÃ¨ne, Thomas Serre.
                  </div>
                  <div class="conference"><span class="badge">NeurIPS</span> Proceedings of the Conference on Neural
                    Information Processing Systems, 2022</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2112.04417">Article</a>

                    <!-- /<a href="https://github.com/deel-ai/xplique">code</a> -->
                  </div>

                  <p class="resume">
                    In this work, we conducted psychophysics experiments at scale to evaluate the ability of human
                    participants to
                    leverage representative attribution methods for understanding the behavior of different image
                    classifiers representing
                    three real-world scenarios: (1) identifying bias, (2) characterizing novel strategy and (3)
                    understanding failure cases.
                    Our results demonstrate that the degree to which individual attribution methods help human
                    participants better
                    understand an AI system varied widely across these scenarios.
                    This suggests a critical need for the field to move past quantitative improvements of current
                    attribution methods.
                  </p>
                </td>
              </tr>

              <!-- HSIC -->
              <tr>
                <td>
                  <div>
                    <img src='images/hsic_miniature.png' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure
                  </div>
                  <div class="authors">
                    Paul Novello, <strong>Thomas Fel</strong>, David Vigouroux
                  </div>
                  <div class="conference"><span class="badge">NeurIPS</span> Proceedings of the Conference on Neural
                    Information Processing Systems, 2022</div>

                  <div class="links">
                    <a href="https://arxiv.org/abs/2206.06219">Article</a>
                    /
                    <a href="https://github.com/deel-ai/xplique">code</a>
                  </div>


                  <p class="resume">
                    This paper could be seen as a follow-up to Look at the variance and further improves the efficiency
                    of black-box
                    methods using Hilbert-Schmidt Independence Criterion (HSIC).
                    HSIC measures the dependence between regions of an input image and the output of a model based
                    on kernel embeddings of distributions.
                    Our experiments show that HSIC is up to 8 times faster than the previous best black-box attribution
                    methods
                    while being as faithful.
                    Importantly, we show that these advances can be transposed to efficiently and faithfully explain
                    object detection
                    models such as YOLOv4.
                  </p>
                </td>
              </tr>

              <!-- Formal -->
              <tr>
                <td>
                  <div>
                    <img src='images/formal_miniature.png'>
                  </div>
                </td>
                <td>
                  <div class="title">
                    Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel*</strong>, Melanie Ducoffe*, David Vigouroux, Remi Cadene,
                    Mikael Capelle, Claire Nicodeme, Thomas Serre
                  </div>
                  <div class="conference"><span class="badge">ICML</span> Workshop on Formal Verification of Machine
                    Learning (Oral), 2022</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2112.04417">Article</a>

                    <!-- /<a href="https://github.com/deel-ai/xplique">code</a> -->
                  </div>

                  <p class="resume">
                    In this work, we propose the first explainability method based on formal method and therefore able
                    to meet certification requirements.
                    Specifically, we introduce EVA (Explaining using Verified perturbation Analysis)
                    -- the first explainability method guarantee to have an exhaustive exploration of a perturbation
                    space.
                    We leverage the beneficial properties of verified perturbation analysis -- time efficiency,
                    tractability
                    and guaranteed complete coverage of a manifold -- to efficiently characterize the input variables
                    that are most
                    likely to drive the model decision.
                    We evaluate the approach systematically and demonstrate state-of-the-art results on multiple
                    benchmarks.

                  </p>
                </td>
              </tr>


              <!-- HKR -->
              <tr>
                <td>
                  <div>
                    <img src='images/hkr_miniature.jpg' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    When adversarial attacks become interpretable counterfactual explanations
                  </div>
                  <div class="authors">
                    Mathieu Serrurier, Franck Mamalet, <strong>Thomas Fel</strong>, Louis BÃ©thune, Thibaut Boissin
                  </div>
                  <div class="conference">Under review, 2022</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2206.06854">Article</a>
                    /
                    <a href="https://github.com/deel-ai/deel-lip">code</a>
                  </div>

                  <p class="resume">
                    We argue that, when learning a 1-Lipschitz neural network with the dual loss of an
                    optimal transportation problem, the gradient of the model is both the direction of
                    the transportation plan and the direction to the closest adversarial attack. Traveling
                    along the gradient to the decision boundary is no more an adversarial attack but
                    becomes a counterfactual explanation, explicitly transporting from one class to
                    the other. Through extensive experiments on XAI metrics, we find that the simple
                    saliency map method, applied on such networks, becomes a reliable explanation!
                    The proposed networks were already known to be certifiably robust, and
                    we prove that they are also tailored for explainability.
                  </p>
                </td>
              </tr>

              <!-- Xplique CVPR -->
              <tr>
                <td>
                  <div>
                    <img src='images/xplique_miniature.png' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    Xplique: A Deep Learning Explainability Toolbox
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel*</strong>, Lucas Hervier*, David Vigouroux, Antonin Poche, Justin Plakoo,
                    Remi Cadene, Mathieu Chalvidal, Julien Colin, Thibaut Boissin, Louis Bethune, Agustin Picard, Claire
                    Nicodeme,
                    Laurent Gardes, Gregory Flandin, Thomas Serre
                  </div>
                  <div class="conference"> <span class="badge">CVPR</span> Workshop on Explainable Artificial
                    Intelligence for Computer Vision, 2022</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2206.04394">Article</a>
                    /
                    <a href="https://github.com/deel-ai/xplique">code</a>
                  </div>
                  <p class="resume">
                    For more than a year, I worked alongside my thesis to implement more than fifty explainability
                    papers.
                    Xplique is the result of this work, it is a library that I develop and maintain.

                    The library is composed of several modules:
                    (1) the Attributions Methods module,
                    (2) The Feature Visualization module,
                    (3) The Concepts module and
                    (4) the Metrics module.
                  </p>
                </td>
              </tr>

              <!-- Sobol NeurIPS -->
              <tr>
                <td>
                  <div>
                    <img src='images/sobol_miniature.png' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel*</strong>,
                    RÃ©mi CadÃ¨ne*, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre.
                  </div>

                  <div class="conference"><span class="badge">NeurIPS</span>Proceedings of the Conference on Neural
                    Information
                    Processing Systems, 2021 </div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2111.04138">Article</a>
                    /
                    <a href="https://github.com/fel-thomas/Sobol-Attribution-Method">code</a>
                  </div>
                  <p class="resume">
                    We describe a novel and efficient black-box attribution method which is grounded in Sensitivity
                    Analysis and uses Sobol indices.
                    Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient
                    way to capture
                    higher-order interactions between image regions and their contributions to a neural network's
                    prediction through
                    the lens of variance.
                  </p>
                </td>
              </tr>

              <!-- Metrics WACV -->
              <tr>
                <td>
                  <div>
                    <img src='images/mege_miniature.png' loading="lazy">
                  </div>
                </td>
                <td>
                  <div class="title">
                    How Good is your Explanation? Algorithmic Stability Measures to Assess the
                    Quality of Explanations for Deep Neural Networks
                  </div>
                  <div class="authors">
                    <strong>Thomas Fel</strong>,
                    David Vigouroux, RÃ©mi CadÃ¨ne, Thomas Serre.
                  </div>

                  <div class="conference"><span class="badge">WACV</span>Proceedings of the IEEE/CVF Winter Conference
                    on
                    Applications of Computer Vision, 2022</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2009.04521">Article</a>
                    /
                    <a href="https://github.com/deel-ai/xplique">code</a>
                  </div>
                  <p class="resume">
                    We propose two new measures to evaluate explanations borrowed from the field of algorithmic
                    stability:
                    mean generalizability MeGe and relative consistency ReCo. We conduct extensive experiments on
                    different network
                    architectures, common explainability methods, and several image datasets to demonstrate the benefits
                    of
                    the proposed metrics and show that popular fidelity measures are not sufficient to guarantee
                    trustworthy explanations.
                    Finally, we found that 1-Lipschitz networks produce explanations with higher MeGe and ReCo than
                    common neural networks.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
  <footer>
    <div>
      <div>Copyright Â© 2020 - 2022 Thomas Fel</div>
      <div>Modified from <a href="https://jonbarron.info/">Jon Barron website</a></div>
    </div>
    <div>
      <a href="mailto:thomas_fel@brown.edu">thomas_fel@brown.edu</a> &nbsp/&nbsp
      <a href="https://scholar.google.com/citations?hl=en&user=1m5Mlx4AAAAJ">Google Scholar</a> &nbsp/&nbsp
      <a href="https://twitter.com/napoolar">Twitter</a> &nbsp/&nbsp
      <a href="https://github.com/fel-thomas/">Github</a>
    </div>
  </footer>
</body>

</html>
